\section*{References}\label{sec:References}

\vspace{10pt}

\begin{enumerateSpaced}{4pt}

    \item Smith, and A., N. (2019, February 19). Contextual Word Representations: A Contextual Introduction. Retrieved from \url{https://arxiv.org/abs/1902.06006}

    \item Melamud, O., Goldberger, et al. (2016). context2vec: Learning Generic Context Embedding with Bidirectional LSTM. \emph{Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning}. doi: 10.18653/v1/k16-1006
    
    \item Devlin, Jacob, et al. (2019, May 24). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Retrieved from \url{https://arxiv.org/abs/1810.04805}
    
    \item Wiedemann, Gregor, et al. (2019, September 23). Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings. Retrieved from \url{https://arxiv.org/abs/1909.10430v1}
    
    \item Munikar, M., et al. (2019). Fine-grained Sentiment Classification using BERT. 2019 \emph{Artificial Intelligence for Transforming Business and Society (AITB)}. doi: 10.1109/aitb48515.2019.8947435
    
    \item Clark, K., et al. (2019). What Does BERT Look at? An Analysis of BERT’s Attention. \emph{Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP}. doi: 10.18653/v1/w19-4828
    
    \item Ethayarajh, K. (2019). How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings. \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).} doi: 10.18653/v1/d19-1006

    \item Batista, D. (n.d.). Language Models and Contextualised Word Embeddings. Retrieved from \url{http://www.davidsbatista.net/blog/2018/12/06/Word_Embeddings/}
    
    \item Neelakantan, A., et al. (2014). Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space. \emph{Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).} doi: 10.3115/v1/d14-1113
    
    \item Antonio, M. (2019, September 5). Word Embedding, Character Embedding and Contextual Embedding in BiDAF - an Illustrated Guide. Retrieved from \url{https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb}
    
    \item Mikolov, T., Sutskever, I., et al. (2013a, October 16). Distributed Representations of Words and Phrases and their Compositionality. Retrieved from \url{https://arxiv.org/pdf/1310.4546.pdf}
    
    \item Mikolov, Tomas, et al. (2013b, September 7). Efficient Estimation of Word Representations in Vector Space. Retrieved from \url{https://arxiv.org/abs/1301.3781}
    
    \item Weng, L. (2017, October 15). Learning Word Embedding. Retrieved from \url{https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html}


    \item Pennington, J., Socher, R., and Manning, C. (2014). Glove: Global Vectors for Word Representation.\emph{Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)}.doi: 10.3115/v1/d14-1162

    
    \item Kurita, K. (2018a, May 4). Paper Dissected: "Glove: Global Vectors for Word Representation" Explained. Retrieved from \url{http://mlexplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/}
    
    \item Sutskever, I., et al. (2014, December 14). Sequence to Sequence Learning with Neural Networks. Retrieved from \url{https://arxiv.org/abs/1409.3215}
    
    \item Vaswani, Ashish, et al. (2017, December 6). Attention Is All You Need. Retrieved from \url{https://arxiv.org/abs/1706.03762}
    
    \item G, R. (2019, March 18). Transformer Explained - Part 1. Retrieved from \url{https://graviraja.github.io/transformer/}
    
    \item Ta-Chun. (2018, October 3). Seq2seq pay Attention to Self Attention: Part 1. Retrieved from \url{https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-1-d332e85e9aad}
    
    \item Alammar, Jay. “Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention).” \emph{Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing Machine Learning One Concept at a Time}, 2018a, \url{jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/}.
    
    \item Alammar, J. (2018b, June 27). The Illustrated Transformer. Retrieved from \url{http://jalammar.github.io/illustrated-transformer/}


    \item Peters, et al. “Deep Contextualized Word Representations.” \emph{ArXiv.org}, (22 Mar. 2018), \url{arxiv.org/abs/1802.05365}.
    
    \item Dai, Zihang, et al. “Transformer-XL: Attentive Language Models beyond a Fixed-Length Context.” \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, (2019), \url{https://arxiv.org/pdf/1901.02860.pdf}.
    
    \item Yang, Z, et al. "XLNet: Generalized Autoregressive Pretraining for Language Understanding." \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, (2020), \url{https://arxiv.org/pdf/1906.08237.pdf}.
    
    \item Kurita, Keita. “Paper Dissected: ‘XLNet: Generalized Autoregressive Pretraining for Language Understanding’ Explained.” \emph{Machine Learning Explained}, (7 July 2019b), \url{mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/}.
    
    
    \item Sun, Y, et al. "ERNIE: Enhanced Representations Through Knowledge Integration." \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, (2019a), \url{https://arxiv.org/pdf/1904.09223.pdf}.
    
    \item Sun, Y, et al. "ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding." \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, (2019b), \url{https://arxiv.org/pdf/1907.12412.pdf}.
    
\end{enumerateSpaced}
    