
\subsection*{\hfil\hfil Abstract \hfil}\label{sec:Abstract}    


\vspace{-5pt}


Knowledge era today churns out and repeats information in the form of documents and media, rather than linking by concept and semantics. However, since concept-mapping enhances human cognition, students and AI-systems could exchange knowledge more successfully and readily using knowledge graphs instead of plain text. This project examines how recent natural language processing (NLP) algorithms use co-occurrences but also more advanced techniques like \hyperref[sec:maskedlanguagemodelMLM]{masking}, \hyperref[sec:autoregressiveLM]{autoregressive language models}, and \nameref{nlptask:namedentityrecognitionNER} to extract lexical, syntactic, and semantic information from real-world text. Models use the \nameref{nlptask:wordsensedisambiguatioNWSD} task to represent \hyperref[sec:Polysemy]{polysemy}, and this is a key step towards concept extraction. Using linked concepts in this document and PyTorch code, we use a fine-grained approach to examine how architectures of state-of-the-art models like \nameref{sec:BERT}, \nameref{sec:XLNet}, and \nameref{sec:ERNIE_1} improve over previous attempts (\nameref{sec:Word2Vec}) in capturing multiple word senses, towards better natural language understanding. 

