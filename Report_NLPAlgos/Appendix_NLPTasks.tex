\section{APPENDIX: Glossary of NLP Tasks} \label{app:Appendix_NLPTasks}

Most of these definitions are from Collobert et al. (2011). 


\subsection{semantic parsing (SP)} \label{nlptask:semanticparsingSP}

\textbf{Semantic parsing (SP)} converts a natural language representation into machine-understanding form. Types include \nameref{nlptask:machinetranslationMT} and \nameref{nlptask:questionansweringQA}.


\subsection{key phrase extraction} \label{nlptask:keyphraseextraction}

\textbf{Key phrase extraction} task uses morphological,  syntactic information, and typed grammar dependencies to learn relevant phrases, and is an important tool for concept extraction. Systems may use \nameref{nlptask:postagging} and \nameref{nlptask:namedentityrecognitionNER} and \nameref{nlptask:namedentityrecognitionNER} to recognize that \texttt{``Air Canada"} is a single entity composed of separate words which only when combined give new meaning (Mikolov et al., 2013a). In some cases, \nameref{nlptask:postagging} uses noun phrases to extract key phrases. 

Key phrase extraction is used in the real world to automate data collection; it results in benefits like data scalability and consistent criteria, so concepts can become used and interlinked at fine-grained levels (``Keyword Extraction", 2019). 



\subsection{machine translation (MT)} \label{nlptask:machinetranslationMT}

\textbf{Machine translation (MT)} task translates an input text in a given language to a target language. There are many population neural translation models like the \nameref{sec:Seq2Seq} and \nameref{sec:BERT} that use the \hyperref[sec:AttentionMechanism]{attention mechanism} to account for contextual meaning across a sentence and not just translate word by word. 

\subsection{neural machine translation (NMT)} \label{nlptask:neuralmachinetranslationNMT}

\textbf{Neural machine translation (NMT)} is \nameref{nlptask:machinetranslationMT} applied in \nameref{sec:NeuralLM}s. 


\subsection{question answering (QA)} \label{nlptask:questionansweringQA}

\textbf{Question answering (QA)} is a task for machines to answer questions posed by humans in a natural language. 


\subsection{semantic role labeling (SRL)} \label{nlptask:semanticrolelabelingSRL}

Also called ``shallow" \nameref{nlptask:semanticparsingSP} , \textbf{semantic role labeling (SRL)} is often described as answering the question ``Who did what to whom?" (Peters et al. 2018). It tries to give a semantic role or tag to a syntactic constituent of a sentence (Collobert et al., 2011).  Examples of semantic roles are \emph{agent, goal} or \emph{result}. 

Specifically, it detects semantics associated to a syntactic sentence feature like predicate or verb and then assigns them semantic roles. 
\begin{itemize}
    \item \textbf{Example Input Sentence: } ``Mary sold the book to John."
    
    \item \textbf{Example Output: } for the verb $\Big[ \text{"to sell"}\Big]_\text{predicate}$; for the noun or argument $\Big[ \text{"Mary"}\Big]_\text{agent}$; for the noun $\Big[ \text{"the book"}\Big]_\text{goods (theme)}$; for the noun $\Big[ \text{"John"}\Big]_\text{recipient}$.
\end{itemize}

SRL can give multiple labels depending on the usage of the syntactic constituent in the sentence. 



\subsection{named entity recognition (NER)} \label{nlptask:namedentityrecognitionNER}

\textbf{Named entity recognition (NER)} is a kind of information extraction task that labels known entities in text into categories like ``Person", ``Location," and ``Organization." \newline 

An \textbf{entity} is a proper noun such as a person, place, or product. A proper noun is more specific than general nouns, which represent more ambigious concepts; for example, ``Emma Watson", ``Eiffel Tower" and ``Second Cup" are entites but the corresponding nouns ``actress", ``architecture" and ``store" are themes.



\subsection{named entity disambiguation (NED)} \label{nlptask:namedentitydisambiguationNED}

\textbf{Named entity disambiguation (NED)} or \textbf{entity linking (EL)} links or maps mentions of an entity (such as persons, locations, companies) within a text to corresponding unique entities in a knowledge base (Shahbazi et al., 2019). 

\begin{itemize}
    \item \textbf{Example input text: } ``Jordan as a  member of the Tar Heels' national championship team."
    
    \item \textbf{Example output: } the language model should predict the named entity (person) ``Michael Jordan", given this exists in the knowledge base, rather than the ambigious mention ``Jordan". 
\end{itemize}

%\subsection{entity extraction (EE)} \label{nlptask:entityextraction}

%\subsection{entity typing (ET)} \label{nlptask:entitytypingET}



\subsection{part of speech tagging (POS)} \label{nlptask:postagging}

\textbf{Part-of-speech tagging (POS)} is the process of labeling each word in a sentence with its part of speech. Every word token is labeled with a tag that identifies its syntactic role (noun, verb, advergb, adjective, ...).  

From (Mohler, 2019): 

\begin{itemize}
    \item \textbf{Example Input Sentence: } ``The tall man is going to quickly walk under the ladder."
    
    \item \textbf{Example Output: } $\Big[ \text{"man"}\Big]_\text{Noun}$, \ $\Big[ \text{"walk"}\Big]_\text{Verb}$, \ $\Big[ \text{"ladder"}\Big]_\text{Noun}$, \ $\Big[ \text{"quickly"}\Big]_\text{Adverb}$ and so on. 
\end{itemize}




\subsection{chunking} \label{nlptask:chunking}

\textbf{Chunking} labels entire pieces of a sentence with tags that indicate their part of speech or syntactic role. For example a phrase can be labeled \emph{noun phrase (NP)}, or \emph{verb phrase (VP)} or even \emph{begin-chunk (B-NP)} and \emph{inside-chunk (I-NP)}. Each \emph{phrase} token is assigned one distinct part of speech tag. 

Chunking operates on the \emph{phrase level} while \nameref{nlptask:postagging} operates on the \emph{word level}. 

From (Mohler, 2019): 
\begin{itemize}
    \item \textbf{Example Input Sentence: } ``The tall man is going to quickly walk under the ladder."
    
    \item \textbf{Example Output: } $\Big[ \text{"the tall man"}\Big]_\text{Noun Phrase}$, \ $\Big[ \text{"is going to quickly walk"}\Big]_\text{Verb Phrase}$, \\ $\Big[ \text{"under the ladder"}\Big]_\text{Preopositional Phrase}$.
\end{itemize}




\subsection{word sense disambiguation (WSD)} \label{nlptask:wordsensedisambiguatioNWSD}

\textbf{Word sense disambiguation (WSD)} identifies the correct word usage from a collection of senses. For a sentences containing \hyperref[sec:Polysemy]{polysemous words}, models use contextual evidence to determine the correct word sense. 



\subsection{lexical substitution} \label{nlptask:lexicalsubstitution}

\textbf{Lexical substitution} substitutes a word given its contextual meaning. For example, the word ``bright" in the phrase ``bright child" can be replaced with ``smart" or ``gifted" rather than ``shining" (Brazinkas et al., 2018). 

\subsection{entailment recognition (ER)} \label{nlptask:entailmentrecognition}

The \textbf{entailment recognition} task is a kind of lexical entailment task or hyponymy detection. Given a pair of words, the task is to predict if the first word $w_1$ entails the second one $w_2$. For (``kiwi", ``fruit"), the task would be to confirm that ``kiwi" entails ``fruit" since it is its hyponym. 


\subsection{textual entailment (TE)} \label{nlptask:textualentailmentTE}

\textbf{Textual entailment (TE)} is the task of determining if a ``hypothesis" is true given a ``premise" (Peters et al., 2018). 

\subsection{entailment directionality prediction} \label{nlptask:entailmentdirectionalityprediction}

Given a pair of words, the \textbf{entailment directionality prediction} task must predict if the previous word entails the next one, or vice versa. It is known that entailment holds for the given word pair and only its directionality is being predicted (Brazinkas et al., 2018). 

\subsection{sentiment classification (SC)} \label{nlptask:sentimentclassificationSC}

See \nameref{nlptask:sentimentanalysisSA}.

\subsection{sentiment analysis (SA)} \label{nlptask:sentimentanalysisSA}

\textbf{Sentiment analysis (SA)} evaluates the sentiment expressed towards an entity (noun or pronoun) based on its proximity to positive or negative words (adjectives and adverbs). For example, a model may classify a movie review as positive, negative or neutral. Generally, SA systems find several attributes of the expression alongside its \emph{polarity}, including the \emph{subject}, the thing being talked about, and \emph{opinion holder}, the entity holding the opinion. 

This task may assign weighted sentiment values to the entities and themes within text. For instance, a hotel getting a review ``astonishing scenery" would get a higher sentiment score than ``banal lake view" because of the stronger adjective ``astonishing." 


\subsection{word similarity} \label{nlptask:wordsimilarity}

The \textbf{word similarity} task determines a similarity score for two input texts. Numerical measures such as cosine similarity compute angular distance between words, based on textual evidence. 


\subsection{word analogy} \label{nlptask:wordanalogy}

The \textbf{word analogy} task completes an analogy. For instance, given the analogy ``meteor" is to ``sky" as ``dolphin" is to <blank>, the task would be to predict a word representing a body of water. 


\subsection{coreference resolution (CR)} \label{nlptask:coreferenceresolutionCR}

\textbf{Coreference resolution (CR)} is the task of collecting all expressions in a text that refer to the same entity in a text.  that refer to the same underlying real world entities. 

\begin{itemize}
    \item \textbf{Example input text: } ``The monkey clambered up the baobab tree and he grabbed a banana and ate it there. The hairy ape screeched while watching the setting sun over the river."
    
    \item \textbf{Example output: } tag the coreferent phrases ``he" and ``the hairy ape" with  ``the monkey"; and also tag ``it" with the same label as ``banana." 
    
\end{itemize}

This task is used as a step towards more general tasks, and is not an end-user task. 

\subsection{sequence labeling (SL)} \label{nlptask:sequencelabelingSL}

\textbf{Sequence labeling (SL)} is a general NLP task that assigns a label to every token in an input sequence, where tokens can be words or phrases. Two forms of sequence labeling include \nameref{nlptask:postagging} and \nameref{nlptask:spanlabeling}

\subsection{span labeling} \label{nlptask:spanlabeling}

In the \textbf{span labeling} task, spans or groups of words are labeled. This can be used in search tasks to provide entities to spans of words for specifying a search query. 

\subsection{semantic textual similarity (STS)} \label{nlptask:semantictextualsimilaritySTS}

\textbf{Semantic textual similarity} determines similarity for two input texts by assigning a score. This measures semantic similarity, so text meaning rather than syntactic similarity. STS differs from both \nameref{nlptask:textualentailmentTE} and paraphrase detection because it detects meaning overlap rather than using a discrete classification of particular relationships. According to Maheshwari et al. (2018), although semantic similarity is characterized by a ``graded semantic relationship", it may be not specify the nature of the relationship since contradictory words still may score highly. For instance, ``night” and ``day” are highly related but contradictory in nature. 



\subsection{tokenization} \label{nlptask:tokenization}

\textbf{Tokenization} or \textbf{segmentation} is the task-specific process of segmenting text into machine-understandable language. The term \emph{tokens} describes words but also punctuation, hyperlinks, and possessive markers, such as apostrophes (Mohler, 2018). For example, lemma-based tokenization would specify that the tokens ``cat" and plural ``cats" would mean one word with the same stem or core meaning-bearing unit. Other forms of tokenization exist to differentiate word form, so those would be distinct tokens. Sentences and even characters can be tokenized out of a paragraph (Chromiak, 2017). Types of tokenization are \textbf{subword tokenization} and \textbf{sentence-piece tokenization}, a key feature in \nameref{sec:TransformerXL}. 


\subsection{transfer learning} \label{nlptask:transferlearning}

\textbf{Transfer learning} or \textbf{domain adaptation} describes how a model created for one task is reused for a different task. Popularly used in machine learning where pre-trained models like \nameref{sec:BERT} are used as starting points for downstream tasks that require much time and resources in order to train. Basically, knowledge of the first task is transferred to the second, often more specific, task (Brownlee, 2017).


\subsection{natural language inference (NLI)} \label{nlptask:naturallanguageinferenceNLI}

\textbf{Natural language inference (NLI)} is the task of predicting whether a \emph{hypothesis} is true, false, or undetermined when the model is given a \emph{premise}. An adapted example from  Ruder (2020) is: 


%\end{comment}

%%%%%
%text summarization (TS)
%semantic dependency parsing (SDP)
%sentence chaining

%entity typing (ET)
%entity type recognition (ETR)
%entity extraction

%subword tokenization (ml)
%sentencepiece tokenization (ml)
