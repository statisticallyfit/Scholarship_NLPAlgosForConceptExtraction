

\begin{frame}{References}


\vspace{10pt}
\begin{enumerateSpaced}{2pt}
    \scriptsize\linespread{0.1} 
    
    \item Smith, and A., N. (2019, February 19). Contextual Word Representations: A Contextual Introduction. Retrieved from \url{https://arxiv.org/abs/1902.06006}

    \item Melamud, O., Goldberger, J., and Dagan, I. (2016). context2vec: Learning Generic Context Embedding with Bidirectional LSTM. \emph{Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning}. doi: 10.18653/v1/k16-1006
    
    \item Devlin, Jacob, Chang, Ming-Wei, Lee, Kenton, … Kristina. (2019, May 24). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Retrieved from \url{https://arxiv.org/abs/1810.04805}
    
    \item Wiedemann, Gregor, Remus, Steffen, Avi, and Chris. (2019, September 23). Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings. Retrieved from \url{https://arxiv.org/abs/1909.10430v1}
    
    \item Munikar, M., Shakya, S., and Shrestha, A. (2019). Fine-grained Sentiment Classification using BERT. 2019 \emph{Artificial Intelligence for Transforming Business and Society (AITB)}. doi: 10.1109/aitb48515.2019.8947435
    
    \item Clark, K., Khandelwal, U., Levy, O., and Manning, C. D. (2019). What Does BERT Look at? An Analysis of BERT’s Attention. \emph{Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP}. doi: 10.18653/v1/w19-4828
    
    
    
    
\end{enumerateSpaced}
    
\end{frame}





\begin{frame}{}

\begin{enumerateSpaced}{2pt}
    \scriptsize
    
     \item Ethayarajh, K. (2019). How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings. \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).} doi: 10.18653/v1/d19-1006

    \item Batista, D. (n.d.). Language Models and Contextualised Word Embeddings. Retrieved from \url{http://www.davidsbatista.net/blog/2018/12/06/Word_Embeddings/}
    
    \item Neelakantan, A., Shankar, J., Passos, A., and Mccallum, A. (2014). Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space. \emph{Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).} doi: 10.3115/v1/d14-1113
    
    \item Antonio, M. (2019, September 5). Word Embedding, Character Embedding and Contextual Embedding in BiDAF - an Illustrated Guide. Retrieved from \url{https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb}
    
    \item Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. (2013a, October 16). Distributed Representations of Words and Phrases and their Compositionality. Retrieved from \url{https://arxiv.org/pdf/1310.4546.pdf}
    
    \item Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. (2013a, October 16). Distributed Representations of Words and Phrases and their Compositionality. Retrieved from \url{https://arxiv.org/pdf/1310.4546.pdf}
    
    
    
    

\end{enumerateSpaced}
    
\end{frame}




\begin{frame}{}
    
    
    \begin{enumerateSpaced}{2pt}
    
        \scriptsize
        
        \item Mikolov, Tomas, Chen, Kai, Corrado, Greg, … Jeffrey. (2013b, September 7). Efficient Estimation of Word Representations in Vector Space. Retrieved from \url{https://arxiv.org/abs/1301.3781}
        
        \item Weng, L. (2017, October 15). Learning Word Embedding. Retrieved from \url{https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html}
        
        \item Pennington, J., Socher, R., and Manning, C. (2014). Glove: Global Vectors for Word Representation.\emph{edings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)}.doi: 10.3115/v1/d14-1162
        
        \item Kurita, K. (2018a, May 4). Paper Dissected: "Glove: Global Vectors for Word Representation" Explained. Retrieved from \url{http://mlexplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/}
        
        \item Sutskever, I., V., Q., and Oriol. (2014, December 14). Sequence to Sequence Learning with Neural Networks. Retrieved from \url{https://arxiv.org/abs/1409.3215}
        
        \item Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, … Illia. (2017, December 6). Attention Is All You Need. Retrieved from \url{https://arxiv.org/abs/1706.03762}
        
        \item G, R. (2019, March 18). Transformer Explained - Part 1. Retrieved from \url{https://graviraja.github.io/transformer/}
        
        \item Ta-Chun. (2018, October 3). Seq2seq pay Attention to Self Attention: Part 1. Retrieved from \url{https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-1-d332e85e9aad}
        
        \item Alammar, J. (2018b, June 27). The Illustrated Transformer. Retrieved from \url{http://jalammar.github.io/illustrated-transformer/}
    
    \end{enumerateSpaced}
    
\end{frame}