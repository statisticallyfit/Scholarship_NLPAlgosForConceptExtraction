
\begin{frame}{}
    \centering \large \textbf{Word Embeddings}
\end{frame}


\begin{frame}{}
    
    \begin{definitionBlock}{Definition: Word Embedding (Words as Vectors)}
        Word embeddings are unsupervised models that capture semantic and syntactic information about words in a compact low-dimensional vector representation $\Rightarrow$ useful for reasoning about word usage and meaning (Melamud et al. 2016, p. 1).
    \end{definitionBlock}
    \vspace{-5pt}
    
    
    \begin{itemizeSpaced}{3pt}
        
        
        \pinkbox Sentence embeddings, phrase embeddings, character embeddings (for morphology)
        
        \pinkbox Can capture \textbf{vector space semantics}: can express word analogy ``man is to woman as king is to queen" with arithmetic on learned word vectors: $vector(man) - vector(woman) = vector(king) - vector(queen)$
        
    \end{itemizeSpaced}
    
    \begin{figure}[h]
    \vspace{-10pt}
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/example_word_embedding.png}
    \vspace{-5pt}
    \caption{\tiny \linespread{0.1} Example Word Embeddings. From \emph{Visualizing Neural Machine Translation Mechanics of Seq2Seq Models with Attention}, by Jay Alammar, 2018. \url{http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/}}
    \vspace{-10pt}
    \label{fig:exampleWordEmb}
    \end{figure}
    
\end{frame}



% \begin{frame}{} %{ \normalsize {\vfill\vfill Mathematically: \\How Models Make Embeddings}}
% 
%   \small\textbf{Mathematically: How Models Make Embeddings ..}
% 
% \begin{itemizeSpaced}{10pt}
%       \small
%     
%     \item a word’s conditional probability combines its \emph{embedding} and \emph{context vectors} of surrounding words, with different methods combining them differently. 
%     
%     \item Then, embeddings are fitted to text by maximizing the conditional probabilities of observed text.
%     
% \end{itemizeSpaced}
%     
% \end{frame}




\begin{frame}{What is Polysemy?}
      \small 
    
    \begin{definitionBlock}{Definition: Polysemy}
    \alert{\textbf{Polysemy}} means a word has multiple senses. 
    \end{definitionBlock}
    
    
    \begin{definitionBlock}{Definition: Distributional Hypothesis}
    \alert{\textbf{Distributional hypothesis}} in NLP says meaning depends on context, and words in same contexts have similar meaning (Wiedemann et al., 2019). 
    \end{definitionBlock}
\end{frame}





\begin{frame}{Static Embeddings}



\begin{itemizeSpaced}{5pt}
    \pinkbox Classic word vectors are called \textbf{static embeddings}. They represent each word with a single vector, regardless of its context (Ethayarajh, 2019). 
    
    
    % Filling block bg colors
    %\metroset{block=fill}
    
    \begin{exampleBlock}{Example}
        \footnotesize
        Skip-Gram and Glove produce these ``context-free" representations because they use co-occurrence counts, not the more dynamic \textbf{language modeling} approach (Batista, 2018). 
    \end{exampleBlock}
    
    \begin{alertBlock}{Alert}
        \footnotesize 
        All senses of a polysemic word are \emph{\alert{collapsed}} within a single vector representation (Ethayarajh, 2019).  
        
        Confusion!  
        
        ``Plant”'s embedding would be the ``average of its different contextual semantics relating to biology, placement, manufacturing, and power generation” (Neelakantan et al., 2015).

    
    \end{alertBlock}
      
\end{itemizeSpaced}
    
\end{frame}




\begin{frame}{Better: Contextual Embeddings (CWE)}


\begin{definitionBlock}{Definition: Contextual Word Embedding}
      \small
    A \textbf{\alert{contextual word embedding (CWE)}} captures context using forward and backward history, using a bidirectional language model (biLM) (Antonio, 2019).   
    
    Static word embeddings are like ``look-up tables" but contextual embeddings have word type information (Smith, 2019). 
\end{definitionBlock}


\begin{itemizeSpaced}{7pt}

    \pinkbox Abandoning the idea of using a fixed word sense inventory (to model polysemy) allows CWE's to create create a vector representation for each \textbf{word type} in the vocabulary and each \textbf{word token} in a context. 
    
    \item Experimentally superior to static embeddings.
    
    \item ``sentence or context-level semantics together with word-level semantics proved to be a powerful innovation” in the NLP world (Wiedemann et al., 2019).
    
    
\end{itemizeSpaced}
    
\end{frame}