
\section{Introduction: Motivation for Text Processing}


\begin{frame}{Introduction: Motivation for Text Processing} %\vspace{40pt}



\begin{itemizeSpaced}{7pt}

    \item Vast amounts of knowledge are trapped in presentation media such as videos, html, pdfs, and paper as opposed to being concept-mapped, interlinked, addressable and reusable at fine grained levels $\Rightarrow$ defeats knowledge exchanges between humans and AI.
    
    \item It is known that concept mapping enhances human cognition. Especially in domain-specific areas of knowledge, better interlinking would be achieved if concepts would be extracted using surrounding context, accounting for polysemy and key phrases. “You shall know a word by the company it keeps” (Firth, 1957).
    
    
    \item Previous count-based models like GloVe and Word2Vec motivated recent models like models like Transformer, ELMo, BERT, Transformer-XL, XLNet, and ERNIE 2.0 to move beyond simple co-occurrence counts to extract meaning. 
    
    \item ERNIE 2.0 instead “broadens the vision to include more lexical, syntactic and semantic information from training corpora in form of named entities (like person names, location names, and organization names), semantic closeness (proximity of sentences), sentence order or discourse relations” (Sun et al., 2019). 
    
    \item \textbf{Example:} ERNIE 1.0 can associate entire entity names with other terms in a given sentence, while on the same data, BERT lacks this ability.
    
    \item \textbf{Aim of This Project: } To understand how models make good language representations via lexical / semantic structure.
    
    
\end{itemizeSpaced}
\end{frame}






% ERASE: 

% \begin{frame}{Aim of this Project}
% 
% To understand how good language representations are created using lexical and semantic structure, at the \emph{entity and phrase level}, using two approaches:
%     
% \vspace{15pt}
% 
% \begin{itemizeSpaced}{15pt}
%     \item \textbf{``Study"} to inventory, compare various architectures and how they leverage entities, polysemy, context for concept extraction. 
%     
%     \item \textbf{``Application"} using PyTorch to illustrate a key model architecture as it is applied to machine translation (MT) task. 
% \end{itemizeSpaced}
%     
% \end{frame}
