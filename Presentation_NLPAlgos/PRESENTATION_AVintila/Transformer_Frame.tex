
\begin{frame}{}
    \begin{center}
        \large \textbf{Transformer}
    \end{center}
    \vspace{20pt}
    
    \textbf{Author(s):}
    \begin{itemizeSpaced}{5pt}
    {\color{DimGrey} 
        \item Vaswani et al. (2017) in \emph{Attention is All You Need}
        
    }
    \end{itemizeSpaced}
\end{frame}

% -------------------------------------------------



\begin{frame}{Transformer: Self-Attention}
    \vspace{15pt}
    
    \begin{itemizeSpaced}{2pt}
        \item Kind of seq-to-seq model for machine translation. More parallelizable than seq-to-seq (no RNNs, just \textbf{self-attention}) to generate sequence of \textit{contextual embeddings}.
        
    \end{itemizeSpaced}
    
    \vspace{-5pt}
    
    \begin{exampleBlock}{Example: Motivation for Self-Attention}
    {\small \emph{``The animal didn't cross the road because it was too tired."}}\newline 
    
    What does ``it" refer to? The road or animal?
    \end{exampleBlock}
    
    \vspace{-10pt}
    \begin{itemizeSpaced}{6pt}
        \pinkbox \textbf{Self-Attention: } \emph{bake in} other word representations into ``it" while processing input (Focus on important words; drown out irrelevant words.)
        
        \item An \textbf{attention function} maps query and key-value pairs to output vector:
        
        \begin{itemizeSpaced}{2pt}
         
            \item Query matrix $Q$ (``it"); Key matrix $K$ rows describe \emph{each} word; Value matrix $V$ rows for all other words (excluding ``it"). 
            
            \item Final output embedding of word is weighted sum: 
            \vspace{-10pt}
            $$
            \texttt{Attention} \Big(Q, K, V \Big) = \texttt{softmax} \Bigg(\frac {QK^T} {\sqrt{d_k}} \Bigg) V
            $$
            
        \end{itemizeSpaced}
        \vspace{-10pt}
        
        \pinkbox In fact, Transformer uses \textbf{multi-head attention mechanism} that comprises of several self-attention heads $\Rightarrow$ Transformer can focus on different words \emph{in parallel.}
        
        
    \end{itemizeSpaced}
\end{frame}


% 
% \begin{frame}{Transformer: Multi-Head Attention}
%     
%     \begin{definitionBlock}{Definition: Multi-Head Attention}
%         \footnotesize  
%         A \textbf{\alert{multi-head attention mechanism}} comprises of several self-attention heads. \newline
%         
%          More attention heads means Transformer can focus on different words; while encoding ``it", one attention head looks at ``the animal" while another focuses on ``tired" $\Rightarrow$ representation of ``it" includes some of all words. \newline 
%          
%         %Enables the Transformer to ``jointly attend to information from different representation subspaces at different positions." 
%         
%         A single attention head cannot do this because of averaging (Vaswani et al., 2017).
%     \end{definitionBlock}
%     
%     \begin{itemizeSpaced}{4pt}
%         \item  Instead of calculating attention once, multi-head attention does (1) self attention many times in parallel on the projected dimensions, (2) concatenates the independent attention outputs, and (3) once again projects the result into the expected dimension to give a final value (Vaswani et al., 2017; Weng, 2018).
%     \end{itemizeSpaced}
%     
% \end{frame}
% 




\begin{frame}{Transformer: Positional Encodings}
    
    \vspace{10pt}
    
    \begin{definitionBlock}{Definition: Positional Encoding}
        \footnotesize 
        
        A \alert{\textbf{positional encoding}} injects absolute token position info so Transformer can see \emph{sentence order} when taking inputs.\newline 
        
        Follows a specific, learned pattern to identify word position or the distance between words in the sequence (Alammar, 2018b). 
        
        $$
        \begin{array}{ll}
        \textit{PosEnc}_{\Large (\textit{pos}, 2i)} = \text{sin} \Bigg(\frac {\textit{pos}} {10000^{\Large \frac {2i} {d_{\textit{model}}} } }  \Bigg) \\
        \textit{PosEnc}_{\Large (\textit{pos}, 2i + 1)} = \text{cos} \Bigg(\frac {\textit{pos}} {10000^{\Large \frac {2i} {d_{\textit{model}}} } }  \Bigg)
        \end{array}
        $$
        where $\textit{pos} = $ a position, $i = $ a dimension.
    \end{definitionBlock}
    
    
    \vspace{-5pt}
    \begin{alertBlock}{Otherwise ...}
    ... ``I like dogs more than cats" and ``I like cats more than dogs" would encode the same meaning (Raviraja, 2019). 
    \end{alertBlock}
    
    
\end{frame}



% ERASE
% \begin{frame}{Transformer: More Layers}
%     \vspace{30pt}
%     
%     \begin{itemizeSpaced}{8pt}
%         \item \textbf{Positionwise feed-forward layer} is a kind of \textbf{feed-forward neural network (FFN)}, and is ``position-wise" since the FFN is applied to each position separately and identically.
%         
%         \item \textbf{Residual Connection: }a sub-layer in Encoder and Decoder stacks for harmonizing gradient optimization procedure.
%         
%         \pinkbox \textbf{Masked Multi-Head Attention: } attention with masking tokens (while decoding word embedding $\overrightarrow{w_i}$, the Decoder is not allowed to see words  $\overrightarrow{w_{>i}}$ past position $i$, only words before $\overrightarrow{w_{\leq i}}$, so no ``cheating" occurs (Ta-Chun, 2018)). 
%         
%         \item \textbf{Encoder: } is bidirectional RNN that concatenates \emph{forward and backward} hidden states to get bidirectional context: $h_t = \Big \{ \overrightarrow{h}_t^T \; ; \; \overleftarrow{h}_t^T \Big\}^T , \: t=1,...,T_x$.\footnotemark 
%         
%         \item \textbf{Decoder: } neural network generates hidden states $s_t = \text{Decoder}\Big( s_{t-1}, y_{t-1}, c_t \Big)$ for times $t = 1,..., m$ \footnotemark  
%         
%     \end{itemizeSpaced}
%     
%     
%     
%     \footnotetext[1]{Note: arrows here denote the direction of the network rather than vector notation.\vspace{-30pt}}
%     
%     \footnotetext[2]{Context vector $c_t = \sum_{i=1}^n \alpha_{ti} \cdot h_i$ is a sum of the hidden states of the input sentence, weighted by alignment scores (same calculation as in the seq-to-seq model)}
%     
% \end{frame}


\begin{frame}{}
    \vspace{10pt}
    
    %\begin{itemizeSpaced}{2pt}
    %    \item Encoder and Decoder are each composed of $N$ identical \textbf{layers} or \textbf{stack}.
    
    %    \item A \textbf{residual connection layer} then layer normalization surround each sub-layer.
    
    %\end{itemizeSpaced}
    
    
    \begin{figure}[h]
    \vspace{-10pt}
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/encoderDecoderLayersDetailed.png}
    \vspace{-5pt}
    \caption{\scriptsize Transformer: Encoder and Decoder Stack in Detail. \textbf{Encoder layer} contains: (1) Multi-head attention, (2) Position-wise feed forward layer. \textbf{Decoder layer} contains: (1) Masked multi-head attention, (2) Encoder-Decoder attention, (3) Position-wise feed forward layer. From \emph{The Illustrated Transformer}, by Alammar, 2018. \url{https://jalammar.github.io/illustrated-transformer/}. Copyright 2018 by Alammar. }
    %\vspace{-5pt}
    \label{fig:encDecLayersDetailed}
    \end{figure}
    
\end{frame}





% ERASE
% \begin{frame}{Transformer: How Do We Predict a Word?}
%     \footnotesize  
%     
%     Generally ...
%     
%     {\color{MediumVioletRed} \textbf{Encoder Stack $\Rightarrow$ Decoder stack $\Rightarrow$ Linear layer {\scriptsize (logits vector)} $\Rightarrow$ Softmax layer {\scriptsize (probabilities vector)} }}
% 
%     \begin{itemizeSpaced}{7pt}
%     \footnotesize 
%         \item \textbf{Linear Layer} neural network projects Decoder's float vector to larger dimension ``logit vector" (each cell holds a score corresponding to each unique vocabulary word.)
%     
%         \item \textbf{Softmax Layer} then converts the Linear Layer's scores into probabilities via the softmax function. 
%     \end{itemizeSpaced}
% 
%     {\color{MediumVioletRed} \textbf{To find the predicted word: }} the cell with highest probability is chosen $\Rightarrow$ its word is the ``predicted" word. 
% 
%     
% \end{frame}