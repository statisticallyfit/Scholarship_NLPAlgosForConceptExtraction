


\begin{frame}{}
    \begin{center}
        \large \textbf{GloVe (Global Vectors for Word Representation)}
    \end{center}
    \vspace{20pt}
    
    \textbf{Author(s):}
    \begin{itemizeSpaced}{5pt}
    {\color{DimGrey} 
        \item Pennington et al. (2014) in \emph{GloVe: Global Vectors for Word Representation}
        
    }
    \end{itemizeSpaced}
\end{frame}

% -------------------------------------------------



\begin{frame}{GloVe: Problem With Word2Vec}
    \vspace{20pt}
    
    \begin{itemizeSpaced}{4pt}
        \pinkbox {\color{Crimson} \textbf{Problem: }} Word2Vec uses local not global counts.  
        \begin{itemizeSpaced}{0pt}
            \pinkbox \textbf{Example: }``the" and ``cat" may occur frequently but Word2Vec does not know if this is because ``the" is a common word or because ``the" and ``cat" are actually correlated  (Kurita, 2018a). 
            
        \end{itemizeSpaced}
        
        \item \textbf{Reason: } 
        \begin{itemizeSpaced}{7pt}
            \pinkbox Word2Vec implicitly optimizes over a co-occurrence matrix while streaming over input sentences $\Rightarrow$ context words are processed equally.
            
            \item Word2Vec's loss function $J = - \sum_i X_i \sum_j P_{ij} \text{log}(Q_{ij}) $ is the cross entropy between the \emph{predicted} and \emph{actual word distributions} in the context of word $i$. \footnote{$X_i = \sum_k X_{ik}$ is the total number of words appearing in the context of word $i$ \newline $Q_{ij} = \text{softmax} \Big( w_i \cdot w_j \Big)$ is the probability that word $j$ appears in context of word $i$} 
            
            \item GloVe: cross entropy models long-tailed distributions poorly, and $X_i$ means equal-weighting over words. 
            
            \pinkbox GloVe: no justification for equal-streaming: (1) use \emph{unnormalized} probabilities, (2) get corpus-wide co-occurrence counts using sliding window.
            
        \end{itemizeSpaced}
        
    \end{itemizeSpaced}
    
    %\footnotetext[3]{\scriptsize $X_i = \sum_k X_{ik}$ is the total number of words appearing in the context of word $i$ \newline $Q_{ij} = \text{softmax} \Big( \texttt{ice} \cdot \texttt{steam} \Big)$ is the probability that word $j$ appears in context of word $i$}
    
\end{frame}




\begin{frame}{GloVe}
\vspace{30pt}


%\begin{itemizeSpaced}{0pt}
%    \item \textbf{Assumption: } words co-occur when they appear within a fixed sliding window $\Rightarrow$ reveals corpus-wide, not sentence-level word meaning.
%\end{itemizeSpaced}

%\vspace{-10pt}
\begin{exampleBlock}{How GloVe Uses Co-Occurrence Ratios \footnotemark}
Consider two words \texttt{ice} and \texttt{steam}. 

\begin{itemizeSpaced}{0pt}
    \item \textbf{Case 1:} For context words related to \texttt{ice} but not ``steam" (\texttt{solid}) $\Rightarrow$ expect co-occurrence probability $p_{\text{co}} \Big( \texttt{solid} \; | \; \texttt{ice} \Big)$ is much larger than $p_{\text{co}} \Big( \texttt{solid} \; | \; \texttt{steam} \Big)  \;\; \Rightarrow \;\; $ ratio $\frac {p_{\text{co}} \Big( \texttt{solid} \; | \; \texttt{ice} \Big)} {p_{\text{co}} \Big( \texttt{solid} \; | \; \texttt{steam} \Big)}$ gets very large.

    \item \textbf{Case 2: } Conversely, for words related to \texttt{steam} but not \texttt{ice} (like \texttt{gas}) $\Rightarrow$ the co-occurrence ratio $\frac {p_{\text{co}} \Big( \texttt{gas} \; | \; \texttt{ice} \Big)} {p_{\text{co}} \Big( \texttt{gas} \; | \; \texttt{steam} \Big)}$ should be small. 

    \item \textbf{Case 3:} For words $w$ related to both \texttt{ice} and \texttt{steam} (like \texttt{water}) or related to neither (like \texttt{fashion}) $\Rightarrow$ expect the ratio $\frac {p_{\text{co}} \Big( w \; | \; \texttt{ice} \Big)} {p_{\text{co}} \Big( w \; | \; \texttt{steam} \Big)}$ is near one. 
\end{itemizeSpaced}
\end{exampleBlock}



\footnotetext[2]{GloVe defines word co-occurrence probability as: 
    $p_{\text{co}} \Big(w_k \; | \; w_i \Big) = \frac{C(w_i, w_k)}{C(w_i)}$  where $C(w_i, w_k)$ counts the co-occurrence between words $w_i$ and $w_k$.  }

\end{frame}



\begin{frame}{Performance: GloVe vs. Word2Vec}
    \vspace{20pt}
    
    \cref{fig:gloveVsWord2vec} shows GloVe's learned embeddings have higher prediction accuracy over those of Skip-Gram and CBOW on \textbf{word analogy} and \textbf{named entity recognition (NER)}.
    
    \begin{figure}[h]
    \vspace{-15pt}
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/table_gloveVSword2vec.png}
    \vspace{-5pt}
    \caption{\linespread{0.1}\tiny Overall accuracy on word analogy task as a function of training time, which is governed by the number of iterations for GloVe and by the number of  negative samples for CBOW (a) and Skip-Gram (b). Pennington et al. (2014) train 300-dimensional vectors on the same 6B token corpus from Wikipedia and use a symmetric context window of size 10. From \emph{GloVe: Global Vectors for Word Representation}, by Pennington et al., 2014. \url{https://nlp.stanford.edu/pubs/glove.pdf}. Copyright 2014 by Pennington et al.}
    \vspace{-5pt}
    \label{fig:gloveVsWord2vec}
    \end{figure}
\end{frame}